# 🔴 OOM 트러블 슈팅 경험

## 🟠 문제 파악 과정

### 🟢 서버가 죽는 이슈 발생

회사에서 정상적으로 잘 돌아가던 프로젝트가 갑자기 주말간 서버가 뻗어버리는 문제가 발생했다.

평일에는 배포가 잦은 프로젝트였기에 메모리 사용량이 모두 차기 전에 재배포가 되어 문제가 발생하지 않았지만 주말간에 메모리 사용량이 점점 증가하다 결국 서버가 뻗어버린 것이다.

다행히 서버가 죽으면 복구 절차가 존재하여 잠시 죽었다가 다시 살아났지만 이는 엄청 큰 문제다.

### 🟢 서버가 죽은 이유

당시 서버의 구성이 한개의 서버에서 총 5개의 어플리케이션이 구동되었다. (그 이유는 내부적인 이유이기 때문에 자세히 말할 순 없다.)

예를 들어 서버의 총 메모리가 16Gb 인데 5개의 어플리케이션이 각각 3Gb*5 였을땐 15Gb로 임계치에 거의 도달했지만 어떻게 운이 좋게 서버가 잘 실행되었다.

APM 도구인 Dynatrace로 확인해보니 정말 임계치에 가깝게 메모리를 사용하고 있었더라... 그러다 특정한 이유로 메모리가 갑자기 튀는 구간이 발생했고 OS에서 메모리 관리를 위해 kill 해버린것이다.

그렇다 보니 서버 로그상에도 아무 이유가 없이 주말간 랜덤하게 서버가 죽어버렸다.

### 🟢 최초 조치

최초 조치로는 서버의 스케일업(scale-up)을 진행했다. AWS를 사용하고 있는데 기존 메모리보다 한단계 윗급의 메모리로 교체했고 다행히도 주말간에 서버가 죽는 이슈는 바로 해결되었다.

하지만 근본적인 문제의 원인을 파악하지 못하였기에 문제를 해결한 것은 아니다. 

또한 swap을 잡아서 굳이 스케일업을 하지 않아도 되지 않았나? 생각할 수 있지만 해당 프로젝트는 속도가 가장 중요한 프로젝트였기에 swap을 잡아 속도를 늦춰 또 다른 문제를 발생시키지 않기 위해 swap 대신 스케일업을 선택했다.

### 🟢 문제 파악

불행 중 다행으로 서버의 메모리가 확보되면서 OOM 문제가 발생하기 시작했다. JVM에 설정된 Heap 메모리 크기보다 더 많은 메모리를 사용하게 되는 부분이 있었으며 JVM 설정으로 OOM 발생시 heap dump하는 설정이 되어 있어 heap dump 파일을 받아서 확인할 수 있었다.

```bash
# jvm에서 oom 발생시 heap dump 설정
-XX:+HeapDumpOnOutOfMemoryError 
```

## 🟠 해결 과정 1

### 🟢 heap dump 분석

heap dump 파일을 분석하기 위해 .hprof 파일을 intellij에서 분석하였고 Retained(메모리에서 차지하는 총 크기) 값이 큰 객체를 찾게 되었다.

LinkedBlockingQueue 메모리가 과다하게 사용됨을 확인할 수 있었다.

### 🟢 코드 레벨 분석

해당 부분에 대해 교체된 소스는 다른 개발팀원으로부터 동시성 문제가 발생하여 기존 List에서 BlockingQueue로 교체되었다는 것을 알고 있었다. 그렇기에 해당 개발자분에게 해당 자료구조를 대체할 수 있는 값이 있을까?에 대해 질문하였고

해당 부분에 대해서는 당장에 대답해줄 수 없을거 같다고 하셨다. 우선은 로직상에서는 기존에 잘 진행되던 로직이고 자료구조만 변경되었는데 발생한 문제이므로 자료구조에 대해 더 알아볼 필요가 있다고 느겼다.

### 🟢 자료구조 파악

자료구조 파악을 위해 [API 문서](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingQueue.html)를 확인해보았고 해당 자료구조의 경우 capacity 값을 파라미터로 넘기지 않을 경우 Integer.MAX의 값으로 생성되어진다는 것을 알수 있었다.

해당 큐 사이즈는 너무 과도하다고 생각되어 해당 부분의 capacity 값을 DB팀과 커뮤니케이션을 통해 100만건으로 설정할 수 있었다.

#### ✅ 상세

```text
LinkedBlockingQueue의 경우 capacity를 설정하면 내부적으로 await을 해주는 로직이 존재한다. 그렇기 때문에 제한된 크기가 설정된다면 큐가 제한된 크기 이상으로 커지지 않고 멈추게 된다.
멈춰있는 동안 GC는 사용되지 않는 객체를 제거하기에 이후 GC가 실행되어질 때, GC에서 탐색하는 자료의 크기가 줄어들어 GC가 빨라진다. GC가 빨리진다면 메모리에 필요 없는 객체를 더 빠르게 제거할 수 있게된다.
그렇기에 큐의 크기를 제한하는 것이 메모리 사용량을 줄일 수 있었던 것이다.

이후에 ArrayBlockingQueue로 변경함으로써 얻은 이득은 Linked 구조는 node를 계속해서 생성해가는 방식에 비해 Array는 정해진 Object[]에 데이터를 넣는 방식이므로 메모리 측면에서 안정성을 더해갈 수 있었다.
단, Linked 구조에 비해 Array는 락이 단일 공유락으로 처리되어 있어 성능면에서는 조금 아쉽다. 성능보단 안정성을 택한 trade off 이다. 
```
 

### 🟢 모니터링

그 결과로 메모리 그래프가 갑자기 튀어 오르는 부분이 사라졌고 총 메모리는 약 4Gb 정도를 절약할 수 있었다.

### 🟢 이후 리팩토링 조치

그 이후로 더 확인해본 결과로 LinkedBlockingQueue는 writeLock과 readLock 2개의 lock을 사용함으로 인해 동시성에서 강한 반면 lock을 2개를 사용해야하기에 메모리 사용량이 추가로 들어갈 수 밖에 없었다. (그 이유는 그만큼 JVM에 lock이 걸려 남아 있는 객체들이 대기하고 있기 때문이 아닐까 싶다.)

같은 BlockingQueue지만 ArrayBlockingQueue의 경우 write와 read를 구분하지 않는 공유 lock을 1개만 사용하고 있기에 동시성에는 LinkedBlockingQueue보다 약하지만 현재 서비스 로직은 write 부분은 멀티 쓰레드지만 read시에는 다시 단일 쓰레드로 작동하도록 로직이 처리되고 있고 빠르게 삽입 후 모두 읽은 후 삭제하고 있기 때문에 LinkedBlockingQueue보단 ArrayBlockingLock이 더 맞는 자료구조라 생각되었다. 또한 capacity 값도 필수로 입력하도록 되어 있어 이후 유지보수에도 더욱 좋을 것으로 판단되었다.

해서 자료구조를 교체하였고 그 결과 총 500Mb 메모리를 추가로 절약할 수 있었다.

## 🟠 해결 과정 2

### 🟢 추가적인 문제

당장 메모리가 급격하게 증가해서 oom이 발생해버리는 문제는 잡을 수 있었지만 APM 도구로 확인했을 때 아직도 메모리는 계속 우상향 그래프를 그렸다. 그래도 서버에서 문제가 발생하지 않는 이유는 평일에는 계속해서 배포가 일어나고 주말간에도 매일 아침마다 새롭게 배포하는 젠킨스 파이프라인이 있었기 때문이다.

### 🟢 heap dump 분석

이번에는 직접 heap dump를 직접 해보았다. 프로그램을 사용하면서 메모리가 계속해서 증가한다는 것은 어디선가 메모리 누수가 발생한다는 것인데 이것을 파악하기 위해서는 계속해서 메모리가 증가하는 객체를 찾아내는게 중요하다고 생각했다.

```bash
jmap -dump:live,format=b,file=test.hprof <pid>
```

다음과 같이 heap dump 파일을 30분 단위로 생성했고 dump를 하는 과정이 실제 서비스에 영향을 줄수도 있다고 보아 사용자들이 서비스를 가장 이용하지 않는 새벽 시간대에 출근하여 dump 파일을 생성해보았다.

해당 파일에서 메모리를 가장 많이 사용하는 부분의 class들 부터 확인해 보기 시작했고 그중 SSLSessionImpl이 가장 의심스러웠다.

### 🟢 SSLSessionImpl 

SSLSessionImpl의 경우 팀내에서 cursor(ai tool)를 사용하고 있었기에 프로젝트에서 내,외부 API 연동을 위해 사용하고 있는 Http Client들을 확인해달라고 했다.

그중 레거시 소스 중 Apache의 Http library를 사용하고 있는 부분이 있었는데. request를 호출하는 부분은 close()를 통해 자원을 반납하고 있었고

response의 경우는 완벽하게 consume되어야 gc에서 처리가 되는데 완전히 consume되지 않아 계속해서 참조 상태로 gc에 처리되지 않을 수 있다고 답변을 받았다.

기존 코드에서 finally에서 consume 하도록 처리하였다. (try-with-resource로 처리하는게 제일 좋았지만 기존 소스들의 사이드 이펙트가 발생할 수 있어 우선적으로 문제를 파악하는게 먼저였기에 finally로 처리하였다)

### 🟢 모니터링

APM 도구(Dynatrace)로 모니터링 결과 그래프가 일정하게 유지되는 그래프를 확인할 수 있었다.

### 🟢 이후 리팩토링

팀에 해당 내용을 공유하였고 팀 리팩토링 기간에 다른 팀원분께서 팀 리팩토링 기간에 테스트와 함께 진행하여 try-with-resource로 전환되었다.